---
title: "tat_dashboard_data"
author: c("Arush Mohan", "Peter Shakeshaft")
date: "`r Sys.Date()`"
params:
  #input_dir: "data/providers"  # Path to the directory where provider folders are stored
  #input_dir: "C:\Users\PeterShakeshaft\OneDrive - NHS\Turnaround Times\Provider Submissions\GOSH"
  submission_month: "Oct24"  # The date of data submission.
format: html
editor: source
---

##Setup

```{r}
#| echo: false

library(renv) 
source('src/functions.R')

```

```{r}
#| echo: false

# Define parameters for the file paths and other variables
input_dir <- c(#r"{C:\Users\PeterShakeshaft\OneDrive - NHS\Turnaround Times\Provider Submissions\GOSH}",
               r"{C:\Users\PeterShakeshaft\OneDrive - NHS\Turnaround Times\Provider Submissions\MEH}",
               #r"{C:\Users\PeterShakeshaft\OneDrive - NHS\Turnaround Times\Provider Submissions\NMUH}",
               r"{C:\Users\PeterShakeshaft\OneDrive - NHS\Turnaround Times\Provider Submissions\RFL}",
               r"{C:\Users\PeterShakeshaft\OneDrive - NHS\Turnaround Times\Provider Submissions\RNOH}",
               r"{C:\Users\PeterShakeshaft\OneDrive - NHS\Turnaround Times\Provider Submissions\UCLH}"
               #r"{C:\Users\PeterShakeshaft\OneDrive - NHS\Turnaround Times\Provider Submissions\Whittington}"
               )

submission_month <- params$submission_month

dsn_name <- "SANDPIT_DSN_TEST"
database_name <- "Data_Lab_NCL_Dev"
schema_name <- "PeterS"
table_name <- "turnaround_times_ROLLTEST"
schema_table_name <- paste(schema_name,table_name,sep = ".")
#full_table_name <- paste(database_name,schema_name,table_name,sep = ".")
db_sch_tbl_name <- paste0(database_name,".",schema_name,".",table_name)

# List all files in the input directory. recursive = FALSE doesn't look in subdirectories (e.g. historical/archive folders)
all_files <- list.files(input_dir, recursive = FALSE, full.names = TRUE, pattern = "*.csv")

# Only read files for selected submission month
all_files <- all_files[grep(submission_month, all_files, perl=T)] 
```


## Import, assign metadata and cleanse data

```{r}

# Process the files into a single dataset

# apply function to list of files
combined_data <- map_dfr(all_files, process_provider_file)

```

## Establishing a connection to the Sandpit

```{r}

# Connect to the Sandpit

con <- dbConnect(odbc::odbc(), 
                 dsn = dsn_name,
                 database = database_name,
                 TrustedConnection = TRUE)
```

## Import lookup tables

```{r}
# lookup_path <- r"{C:\Users\PeterShakeshaft\Documents\Projects\tat_dashboard\data\lookups}"
# lookup_file_name <- list.files(lookup_path)
# lookup_full_path <- paste(lookup_path, lookup_file_name, sep = "\\")
# 
# # function to read all sheets within an excel file to separate data frames
# read_excel_allsheets <- function(filename, tibble = FALSE) {
#   sheets <- readxl::excel_sheets(filename)
#   x <- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X)) 
#   if(!tibble) x <- lapply(x, as.data.frame)
#   names(x) <- sheets
#   x <- x %>% janitor::clean_names(case = "snake")
# }
# 
# lookups <- read_excel_allsheets(lookup_full_path)
# 
# # turns from list into separate data frames
# list2env(lookups, envir=.GlobalEnv)
# 
# # query to pull organisation lookup table
# org_query <- "SELECT
#               Organisation_Code
#               ,LEFT(Organisation_Code,3) 'Organisation_Trust_Code'
#               ,Organisation_Name
#               ,SK_OrganisationTypeID
#               FROM [Dictionary].[dbo].[Organisation]
#               where 
#               SK_OrganisationTypeID IN (41,42)"
# 
# lookup_organisation <- dbGetQuery(con, org_query) %>% janitor::clean_names(case = "snake")


```

## Wrangle and de-normalise dataset for final output

```{r}

# 1. read in lookup tables.
# 2. do joins
# 3. do calced fields
# 4. do selects
# 
# wrangled_data <- combined_data %>% 
#   left_join(lookup_patient_source_setting, by = join_by(patient_source_type == Code)) %>% 
#   left_join(lookup_combined_image_code, by = join_by(combined_imaging_code == Code)) %>% 
#   left_join(lookup_targets, by = join_by(priority_type_code_routine_default == Code))

# Pare down dataset to just essential columns
combined_data_pared <- combined_data %>% select(diagnostic_test_date_time,
                                                # diagnostic_test_request_date_time,
                                                service_report_issue_date_time,
                                                patient_source_type,
                                                priority_type_code,
                                                imaging_code_nicip,
                                                imaging_code_snomed,
                                                provider_site_code,
                                                TAT_scan,
                                                TAT_report,
                                                TAT_overall,
                                                data_type,
                                                data_period
                                                )

#View(combined_data %>% filter(is.na(diagnostic_test_date_time)))
```


## Upload data to sandpit

```{r}
# Upload using overwrite just to get some data in
                                                
# DBI::dbWriteTable(con, DBI::SQL(schema_table_name), combined_data_pared, row.names=F, overwrite = TRUE) # write to new main table
# print(paste0(db_sch_tbl_name," overwritten with new data"))

```

## Upload data to sandpit - rolling flex/freeze

```{r}
#1. delete flex data from previous month

delete_query <- paste0("DELETE FROM ", schema_table_name,
                       " WHERE data_type = 'Flex'")

# Execute the delete query
dbExecute(con, delete_query)
message("Flex data deleted.")

#2. upload new month
DBI::dbWriteTable(con, DBI::SQL(schema_table_name), combined_data_pared, row.names=F, append = TRUE) # write to new main table
message("Data appended for ", submission_month,"." )
```

